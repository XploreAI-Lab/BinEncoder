Stage 1: Batch Microcode Extraction
Use ida_wakeup.py and info_collect.py to extract function microcode and metadata from the original binary files (.elf).

Stage 2: Build Tokenizer
extract_token.py: Processes the data and generates an add_vocal.txt file containing all custom microcode tokens.
build_mytokenizer.py: Loads a pre-trained BERT model, adds the new tokens from add_vocal.txt, and saves a new model and tokenizer directory ready for training.

Stage 3: Pre-processing and Serialization
pretoken.py: Reads the raw JSON generated in Stage 1 and extracts only nverbs (instructions) and edges (CFG).
gen_seq_json.py: Reads the files generated by pretoken.py and converts the nverb list into a model-readable, comma-separated instruction string.

Stage 4: Pre-training
gen_pair.py: Generates pos_pairs.json and neg_pairs.json based on the CFG.
bert_finetune.py: Loads the BertForPreTraining model and simultaneously trains it on the MLM, JDP , and VDBP  tasks.

Stage 5: Contrastive Learning
triplet.py: Scans all functions and generates (anchor, positive, negative) tuples based on function hash.
contrastive.py: Loads the pre-trained BertModel and fine-tunes it using a cosine similarity triplet loss (compute_triplet_loss).

Stage 6: Inference and Evaluation
eval_seq.py: Performs the same serialization operation as gen_seq_json.py for the evaluation dataset.
gen_vec.py: Loads the fine-tuned model, performs inference on all evaluation sequences, and generates final function embeddings using mean pooling.
add_funcname.py: Merges the metadata JSON containing function names with the embedding JSON for evaluation.
eval.py: Loads all named embeddings, groups them by function name, and calculates MRR and Recall@1 between different compilation options (e.g., O0 vs O3).
