IDA Pro Feature Extractor
This toolset automates feature extraction from binary files using IDA Pro. It is designed to batch-process a large number of binaries (.elf, .idb, .i64) and save the extracted features into a JSON format for downstream tasks.

Core Components
1. Batch Launchers (ida_wakeup.py, ida_a.py)

These scripts automatically find and launch the IDA Pro executable (idat.exe / idat64.exe) in autonomous mode (-A -S).

They iterate over a directory of binaries, detect the architecture (32/64-bit), and pass the correct analysis script to IDA.

ida_a.py also includes logic to pre-process .idb files into .i64 databases before analysis.

2. IDAPython Extractors (info_collect.py, info_collect_jtrans.py, idb_to_json.py)

These scripts run inside IDA Pro to perform the analysis.

They iterate through all functions in the .text segment of the binary.

Key features extracted include:

Microcode: Both original (ori_mc) and "regularized" (regu_mc) intermediate representations.

Call Graph: Function callers (caller) and callees (callee).

Function Metadata: Function names (fname) and local variables (mc_vars, nice_name).

All extracted data is serialized and saved as JSON files in an output directory (e.g., call_graph_new_json or RTOS-525-json).

Model Training and Evaluation Pipeline
This set of scripts takes the JSON files generated by the IDA Feature Extractor and processes them through a complete machine learning pipeline, including data pre-processing, model training, embedding generation, and evaluation.

1. Data Pre-processing and Tokenization
Before training, the raw features extracted from IDA must be converted and tokenized into a format the model can understand.

Feature Filtering (pretoken.py)

This script first reads data from the raw JSON feature files.

It parses the data, specifically extracting nverbs (node features/instructions) and edges (control flow) from the ISCG (Internal Semantic Control-flow Graph).

It saves this filtered information into new JSON files for subsequent processing.

P-code Serialization (gen_seq_json.py)

This is a key tokenization script that reads the files generated by pretoken.py.

It uses the parse_nxopr function to parse P-code style instruction strings.

It converts each function's nverb list into a single, comma-separated string of instruction sequences.

The final output is a JSONL file where each line represents one function's tokenized instruction sequence.

Human-Readable Dump (get_fit_all_inst.py)

This is a utility script used to dump the contents of nverb and edges into a human-readable .txt format, primarily for debugging and validation.

2. Model Training
This project demonstrates two different training strategies:

A. Masked Language Model (MLM) + Next Sentence Prediction (NSP) Pre-training
Training Pair Generation (gen_pair.py)

This script creates training data for the NSP task.

Positive Pairs: By traversing the function's control flow graph (edges), instruction pairs that are adjacent in the CFG are used as positive samples.

Negative Pairs: Negative samples are created by randomly sampling two non-adjacent instructions from the same function.

The results are saved as pos_pairs.json and neg_pairs.json.

BERT Fine-tuning (bert_finetune.py)

This is the core training script, which loads a pre-trained BERT model (BertForPreTraining).

It loads the positive and negative pairs generated by gen_pair.py.

The TrainDataset class implements two tasks simultaneously:

MLM: The mask_tokens function randomly masks tokens in the input sequence.

NSP: Uses the loaded "sentence pairs" (instruction pairs) and their labels (0 for adjacent, 1 for non-adjacent) for training.

The script uses the AdamW optimizer to jointly train the model and saves the fine-tuned model to disk.

B. Triplet Loss Contrastive Learning
Triplet Generation (triplet.py)

This script generates triplet training data for contrastive learning.

It scans all functions and groups them by function hash (assuming same hash = same source code, different compilation).

It generates triplets (anchor, positive, negative):

Anchor: A function sample.

Positive: Another compiled version of the same function.

Negative: A random sample of a different function.

The result is saved as a JSONL file for contrastive learning.

3. Inference and Embedding Generation
Vector Generation (gen_vec.py)

This script loads the trained BertModel (note: not BertForPreTraining).

It reads the serialized JSONL files generated by gen_seq_json.py.

For each function sequence, it performs inference with the BERT model and uses mean pooling (outputs.last_hidden_state.mean(dim=1)) to generate the function's embedding.

The final output is a JSONL file, with each line containing a function identifier and its corresponding vector.

4. Evaluation and Visualization
Similarity Search (search.py)

This is an evaluation script used to test the quality of the embeddings.

It loads the embedding file from gen_vec.py and computes the cosine_similarity between a given target function and all other functions.

It outputs the function with the highest similarity (i.e., the best match).

t-SNE Visualization (T-sne.py)

This script uses sklearn.manifold.TSNE to reduce the high-dimensional function embeddings into 2D space.

It uses matplotlib.pyplot to plot a scatter graph, visualizing the clustering of functions in the embedding space.

Metadata Enrichment (add_funcname.py)

This is a helper utility to add human-readable function names (func_name) to the embedding files.

It merges information from two different JSON sources by matching function addresses.
